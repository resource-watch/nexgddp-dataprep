{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from dask import delayed\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "import glob\n",
    "import tempfile\n",
    "import subprocess\n",
    "import datetime, time\n",
    "from urllib import request\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "import gc\n",
    "import boto3\n",
    "import botocore\n",
    "import itertools\n",
    "\n",
    "storage_client = storage.Client.from_service_account_json('/home/jovyan/work/credentials.json')\n",
    "bucket = storage_client.get_bucket('nex-gddp')\n",
    "\n",
    "loca_bucket = 'nasanex'\n",
    "base_key_path = 'LOCA'\n",
    "base_url = 'ftp://gdo-dcp.ucllnl.org/pub/dcp/archive/cmip5/loca/LOCA_2016-04-02/'\n",
    "all_models = [\"ACCESS1-0\",\"ACCESS1-3\",\"CCSM4\",\"CESM1-BGC\",\"CESM1-CAM5\",\"CMCC-CM\",\"CMCC-CMS\",\"CNRM-CM5\",\"CSIRO-Mk3-6-0\",\"CanESM2\",\"EC-EARTH\",\"FGOALS-g2\",\"GFDL-CM3\",\"GFDL-ESM2G\",\"GFDL-ESM2M\",\"GISS-E2-H\",\"GISS-E2-R\",\"HadGEM2-AO\",\"HadGEM2-CC\",\"HadGEM2-ES\",\"IPSL-CM5A-LR\",\"IPSL-CM5A-MR\",\"MIROC-ESM\",\"MIROC-ESM-CHEM\",\"MIROC5\",\"MPI-ESM-LR\",\"MPI-ESM-MR\",\"MRI-CGCM3\",\"NorESM1-M\",\"bcc-csm1-1\",\"bcc-csm1-1-m\",\"inmcm4\"]\n",
    "some_models = [\"ACCESS1-0\",\"ACCESS1-3\",\"CCSM4\"]\n",
    "\n",
    "#client = Client('scheduler:8786')\n",
    "dask.set_options(get=dask.get)\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "\n",
    "# Begin here\n",
    "def process_model_year(model, scenario, year):\n",
    "    temps = process_year_temps(model, scenario, year)\n",
    "    return temps\n",
    "\n",
    "def process_year_temps(model, scenario, year):\n",
    "    print(f\"Processing temperatures for {model} {year} ({scenario})\")\n",
    "    ids = (gen_netcdf_id(model, scenario, year, 'tasmax'), gen_netcdf_id(model, scenario, year, 'tasmin'))\n",
    "    print(f\"File ids are: {ids}\")\n",
    "    tasmax_file, tasmin_file = list(map(download_file, ids))\n",
    "    tasmax_dataset, tasmin_dataset = netCDF4.Dataset(tasmax_file), netCDF4.Dataset(tasmin_file)\n",
    "    tasmax_arr = da.from_array(tasmax_dataset['tasmax'], chunks = (365, 490, 960))\n",
    "    tasmin_arr = da.from_array(tasmin_dataset['tasmin'], chunks = (365, 490, 960))\n",
    "    tasavg_arr = np.mean(da.stack((tasmax_arr, tasmin_arr)), axis = 0)\n",
    "    \n",
    "    # Annual means\n",
    "    print(\"Calc avg_tasmin\")\n",
    "    avg_tasmin = np.mean(tasmin_arr, axis = 0).compute()\n",
    "    print(avg_tasmin)\n",
    "    print(\"Calc avg_tasmax\")\n",
    "    avg_tasmax = np.mean(tasmax_arr, axis = 0).compute()\n",
    "    print(avg_tasmax)\n",
    "    print(\"Calc avg_tasavg\")\n",
    "    avg_tasavg = np.mean(tasavg_arr, axis = 0).compute()\n",
    "    print(avg_tasavg)\n",
    "    \n",
    "    hdds = delayed(hdd)(tasavg_arr, axis = 0).compute()\n",
    "    cdds = delayed(cdd)(tasavg_arr, axis = 0).compute()\n",
    "    ffs = delayed(frost_free_season)(tasmin_arr, axis = 0).compute()\n",
    "    \n",
    "    res = np.stack((\n",
    "        avg_tasmax,\n",
    "        avg_tasmin,\n",
    "        avg_tasavg,\n",
    "        hdds,\n",
    "        cdds,\n",
    "        ffs\n",
    "    ))\n",
    "    \n",
    "    output_filename = f'{year}_{model}_processed_temperatures.npy'\n",
    "    np.save('/temp/' + output_filename, results)\n",
    "    blob = bucket.blob(output_filename)\n",
    "    blob.upload_from_filename('/temp/' + output_filename)\n",
    "    os.remove('/temp/' + output_filename)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def gen_netcdf_id(model, scenario, year, var):\n",
    "    id = f'LOCA/{model}/16th/{scenario}/r1i1p1/{var}/{var}_day_{model}_{scenario}_r1i1p1_{str(year)}0101-{str(year)}1231.LOCA_2016-04-02.16th.nc'\n",
    "    return id\n",
    "\n",
    "def download_file(file_id, loca_bucket = loca_bucket, download_location = '/temp'):\n",
    "    filename = f'{download_location}/{file_id.split(\"/\")[-1]}'\n",
    "    print(f\"Downloading {filename}\")\n",
    "    try:\n",
    "        s3.Bucket(loca_bucket).download_file(file_id, filename)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            file_id = file_id.replace('r1i1p1', 'r6i1p1')\n",
    "            s3.Bucket(loca_bucket).download_file(file_id, filename)\n",
    "    except:\n",
    "        filename = None\n",
    "    return filename\n",
    "\n",
    "def cleanup():\n",
    "    for file in glob.glob('/temp/*'):\n",
    "        os.remove(file)\n",
    "\n",
    "# Actual processing\n",
    "def hdd(a, axis):\n",
    "    a_to_baseline = 291.483 - a\n",
    "    masked = ma.masked_where(a_to_baseline <= 0, a_to_baseline)\n",
    "    intermediate_matrix = ma.filled(masked, fill_value = 0)\n",
    "    result = np.sum(intermediate_matrix, axis = 0)\n",
    "    return result\n",
    "\n",
    "# Cooling degree days\n",
    "def cdd(a, axis):\n",
    "    a_to_baseline = 291.483 - a\n",
    "    a_to_baseline[a_to_baseline < -10000] = 0\n",
    "    masked = ma.masked_where(a_to_baseline >= 0, a_to_baseline)\n",
    "    intermediate_matrix = ma.filled(masked, fill_value = 0)\n",
    "    result = np.sum(np.abs(intermediate_matrix), axis = 0)\n",
    "    return result\n",
    "\n",
    "def longest_streak(diff):\n",
    "    result = 0\n",
    "    try:\n",
    "        result =  np.amax(\n",
    "            np.array(np.where(diff < 0)) - np.array(np.where(diff > 0))\n",
    "        )\n",
    "    except ValueError:\n",
    "        #raised if empty\n",
    "        result = 0\n",
    "    return result\n",
    "\n",
    "# Longest streak of days over freezing temperature (tasmin)\n",
    "def frost_free_season(a, axis):\n",
    "    # First, dealing with the first matrix\n",
    "    frost_days_matrix = (a > 273.15) * 1\n",
    "    # We pad it with zeroes at the ends of the designed axis\n",
    "    zeros_shape = list(a.shape)\n",
    "    del zeros_shape[axis]\n",
    "    zeros_matrix = np.expand_dims(np.zeros(zeros_shape), axis = axis)\n",
    "    concat_matrix = np.concatenate((zeros_matrix, frost_days_matrix, zeros_matrix))\n",
    "    # We calculate the deltas along an axis\n",
    "    diff = np.diff(concat_matrix, axis = axis)\n",
    "    # And get the longest streak from there --\n",
    "    # apply along axis is far from ideal, but\n",
    "    # np.where doesn't operate over axes, so we have to iterate\n",
    "    result = np.apply_along_axis(longest_streak, axis, diff)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing temperatures for ACCESS1-0 1971 (historical)\n",
      "File ids are: ('LOCA/ACCESS1-0/16th/historical/r1i1p1/tasmax/tasmax_day_ACCESS1-0_historical_r1i1p1_19710101-19711231.LOCA_2016-04-02.16th.nc', 'LOCA/ACCESS1-0/16th/historical/r1i1p1/tasmin/tasmin_day_ACCESS1-0_historical_r1i1p1_19710101-19711231.LOCA_2016-04-02.16th.nc')\n",
      "Downloading /temp/tasmax_day_ACCESS1-0_historical_r1i1p1_19710101-19711231.LOCA_2016-04-02.16th.nc\n",
      "Downloading /temp/tasmin_day_ACCESS1-0_historical_r1i1p1_19710101-19711231.LOCA_2016-04-02.16th.nc\n",
      "Calc avg_tasmin\n",
      "[[  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " ..., \n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]]\n",
      "Calc avg_tasmax\n",
      "[[  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " ..., \n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]]\n",
      "Calc avg_tasavg\n",
      "[[  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " ..., \n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]\n",
      " [  1.00000085e+30   1.00000085e+30   1.00000085e+30 ...,   1.00000085e+30\n",
      "    1.00000085e+30   1.00000085e+30]]\n"
     ]
    }
   ],
   "source": [
    "res = process_year_temps(\"ACCESS1-0\", \"historical\", 1971)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-30c3523edfe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lower'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 6"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "res[res > 100000] = 0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(res[6, :, :], origin = 'lower')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GET /process/:year/:model/:scenario\n",
    "req = json.loads(REQUEST)\n",
    "year = int(req['path']['year'])\n",
    "model = req['path']['model']\n",
    "scenario = req['path']['scenario']\n",
    "cleanup()    \n",
    "result = download_and_process(model, scenario, year)\n",
    "print({'output': result})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
